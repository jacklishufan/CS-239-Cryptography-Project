# -*- coding: utf-8 -*-
"""LWE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kIjCjSotu99eWtA5XJWk-Ueydlsnursf
"""

import numpy as np
import torch
def discrete_gaussian(n, sigma):
    """
    Generate a discrete Gaussian noise vector.

    :param n: Number of samples.
    :param sigma: Standard deviation of the Gaussian distribution.
    :return: Discrete Gaussian noise vector of size n.
    """
    # Step 1: Sample from continuous Gaussian
    continuous_samples = np.random.normal(loc=0, scale=sigma, size=n)

    # Step 2: Round to nearest integers to make discrete
    discrete_samples = np.round(continuous_samples).astype(int)

    return discrete_samples

def generate_keys(m,n,q):
    s = np.random.randint(0, q, size=n,dtype=np.uint64) # n
    A = np.random.randint(0, q, size=(m, n),dtype=np.uint64) # m x n
    e = np.random.randint(0,32,size=m,dtype=np.uint64) # m
    key = (A @ s + e) % q  # m
    return A, key, s

def encrypt(A, key, plaintext_vector):
    scale_factor = q // p
    v = (scale_factor * plaintext_vector + key) % q
    return v

def decrypt(A, s, q, p, ciphertext_vector):
    scale_factor = q // p
    u = (A @ s ) % q
    decrypted = (ciphertext_vector - u) % q
    print(decrypted.dtype)
    plaintext = ((decrypted+scale_factor//2) // scale_factor) % p  # Scale back to plaintext space
    return plaintext

def add_plaintext_constant(ciphertext_vector, constant, q, p):
    scale_factor = q // p
    scaled_constant = (scale_factor * constant) % q
    return (ciphertext_vector + scaled_constant) % q

def multiply_ciphertext_constant(ciphertext, constant, key, q):
    """
    Multiply an LWE ciphertext by a plaintext constant.

    :param ciphertext: The encrypted vector (ciphertext).
    :param constant: The plaintext constant to multiply.
    :param A: Encryption matrix.
    :param e: Noise vector.
    :param q: Modulus for the LWE encryption.
    :return: Updated ciphertext, adjusted A, and noise vector.
    """
    # Scale ciphertext
    ciphertext_scaled = (ciphertext * constant) % q

    # Scale A and noise to match multiplication
    key_scaled = (key * constant ) % q

    return ciphertext_scaled, key_scaled

import math

# Maps #samples to plaintext modulus for logq < 64
plaintextModulus32 = {
    1 << 13: 991,
    1 << 14: 833,
    1 << 15: 701,
    1 << 16: 589,
    1 << 17: 495,
    1 << 18: 416,
    1 << 19: 350,
    1 << 20: 294,
}

# Maps #samples to plaintext modulus for logq = 64
plaintextModulus64 = {
    1 << 13: 574457,
    1 << 14: 483058,
    1 << 15: 406202,
    1 << 16: 341574,
    1 << 17: 287228,
    1 << 18: 241529,
    1 << 19: 203101,
    1 << 20: 170787,
    1 << 21: 143614,
    # Uncomment the following if needed
    # 1 << 22: 120764,
    # 1 << 23: 101550,
    # 1 << 24: 85393,
    # 1 << 25: 71807,
    # 1 << 26: 60382,
    # 1 << 27: 50775,
}

def new_params(logq, nSamples):
    """
    Determine LWE parameters for Regev encryption.

    :param logq: Logarithm of ciphertext modulus (in bits).
    :param nSamples: Number of homomorphic additions to support.
    :return: Dictionary of parameters or None if no valid parameters are found.
    """
    max_value = float('inf')
    m = max_value
    pmod = 0

    # Select appropriate modulus options
    options = plaintextModulus32
    if logq == 64:
        options = plaintextModulus64

    # Find the smallest m that supports nSamples
    for m_new, p_new in options.items():
        if m_new < m and nSamples <= m_new:
            m = m_new
            pmod = p_new

    # If no valid parameters are found, return None
    if m == max_value:
        return None

    # Return parameters
    return {
        "logq": logq,
        "m": m,
        "pmod": pmod
    }


# Parameters
n = 1024
logq = 64
q = 2**logq
param = new_params(logq,1024)
p = param['pmod']  # Plaintext modulus
print(param)
# Keys
m = param['m']
A, pk, sk = generate_keys(m,n, q)

# Example plaintext
plaintext_vector = np.array([1, 3, 5, 6]).astype(np.uint64)  # Elements modulo p
plaintext_vector = np.concatenate([plaintext_vector,np.zeros(m-len(plaintext_vector))]).astype(np.uint64)
print(plaintext_vector,plaintext_vector.dtype)
# Encrypt
ciphertext = encrypt(A, pk, plaintext_vector)
print(ciphertext,ciphertext.dtype)
updated_ciphertext = ciphertext
# Add constant
constant_to_add = 2
#updated_ciphertext = add_plaintext_constant(ciphertext, constant_to_add, q, p)
#A,e,updated_ciphertext = multiply_ciphertext_constant(ciphertext, constant_to_add,A,e ,q)
#updated_ciphertext = updated_ciphertext * 2
# A = A * 2
# e = e * 2
# A *= constant_to_add
# Decrypt
updated_ciphertext[0] = (updated_ciphertext[0]+updated_ciphertext[1])%q
A[0] = (A[0] + A[1])%q
decrypted_vector = decrypt(A, sk, q, p, updated_ciphertext)
# decrypted_vector[1]=1
# Debug Outputs
print("Plaintext Vector:", plaintext_vector)
print("Constant to Add:", constant_to_add)
print("Ciphertext Before Adding Constant:", ciphertext)
print("Ciphertext After Adding Constant:", updated_ciphertext)
print("Decrypted Vector:", decrypted_vector)
# Very simple sanity check

ciphertext.shape

def sample_from_unit_sphere(dim, num_samples=1):
    # Generate random points
    samples = np.random.normal(0, 1, (num_samples, dim))
    # Scale them to be within the unit sphere
    radii = np.random.uniform(0, 1, num_samples) #** (1 / dim)
    # Normalize the samples to the surface of the sphere and scale by radius
    samples = samples / np.linalg.norm(samples, axis=1, keepdims=True) * radii[:, np.newaxis]
    return samples

################## SERVER PREPROCESSING #################################
QF = 32 # quantize factor for fixed point ops
N_docs = 1000
# Although we generat pk,sk, only A is used in server side preprocessing, (A is "hint")
A, pk, sk = generate_keys(m,n, q)
d = 768
emb = sample_from_unit_sphere(d,N_docs) # 1000,1023
emb_norm = np.linalg.norm(emb,axis=-1,keepdims=True)**2 # D X L
gamma = 1 / (1-emb_norm+1e-4)
beta = emb_norm * gamma
emb_scaled = -2 * emb * gamma

###################################################
emb_scaled = (emb_scaled * QF) # round to fixed point int
emb_scaled = emb_scaled % p
emb_scaled = emb_scaled.astype(np.uint64)

beta = (beta * QF * QF ) # round to fixed point int
beta = beta % p
beta = beta.reshape(-1)
beta = beta.astype(np.uint64)
AD = (A[:d].T @ emb_scaled.T).T # N X N
AD = AD % q #
gamma_fac = (gamma * QF).reshape(-1)
gamma_fac = ( gamma_fac % p).astype(np.uint64)
A_addition = A[d:d+1] * gamma_fac.reshape(-1,1)
AD_final = AD + A_addition





#AD.shape

query = (sample_from_unit_sphere(d,1))
raw_query = query
query_norm = np.linalg.norm(query,axis=-1,keepdims=True)**2
query = (query*QF) % p
query = query.astype(np.uint64)
#################CLIENT ###########################
# encrypt query q
ciphertext = encrypt(A[:d], pk[:d], query[0]) # 2048


query_norm = (query_norm*QF ) % p
query_norm = query_norm.astype(np.uint64)
ciphertext_norm = encrypt(A[d:d+1], pk[d:d+1], query_norm[0]) # N, 1

#################Server Handling ###########################
# add gamma * utu
scores = ((emb_scaled @ ciphertext) + beta * q // p ) % q


#gamma_fac = gamma_fac * 0 + 1
score_addition = (ciphertext_norm * gamma_fac) % q


scores_final = (scores + score_addition) % q

#ciphertext_norm

#########Decryption of final score
decrypted_scores_all = decrypt(AD_final, sk, q, p, scores_final)
ref_score_final = ((emb_scaled @ query[0] ) + beta + gamma_fac * query_norm ) % p
# print(decrypted_scores_all)
# print(ref_score_final)

# Check only term beta - 2 u^tv
ref_score = ((emb_scaled @ query[0] ) + beta ) % p
decrypted_scores = decrypt(AD, sk, q, p, scores)
# print(ref_score)
# print(decrypted_scores)
print("Diff",ref_score-decrypted_scores)
# decrypted_scores, ((emb_scaled @ query[0] ) + beta +gamma_fac * query_norm) % p

from matplotlib import pyplot as plt

true_score= ((emb- raw_query)**2).sum(-1) / (1 - (emb ** 2).sum(-1) + 1e-4) * QF * QF

np.argsort(true_score) == np.argsort(decrypted_scores_all)

plt.plot(sorted(true_score),sorted(decrypted_scores_all))
plt.xscale('log')
plt.yscale('log')
# plt.plot(decrypted_scores)
# while not accurate, this is monotonicaly increasing

# true_dist =

# Check only term gamma * u^Tu
ref_score_add = gamma_fac * query_norm
decrypted_scores_add = decrypt(A_addition, sk, q, p, score_addition)
# print(ref_score_add[:10])
# print(decrypted_scores_add[:10])
print(ref_score_add-decrypted_scores_add)